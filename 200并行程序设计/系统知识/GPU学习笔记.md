---
date: 2025-05-05
tags:
  - study
---
[CUDA编程基础(一)—前言 - 皮皮鲁的文章 - 知乎 https://zhuanlan.zhihu.com/p/4127619601](https://zhuanlan.zhihu.com/p/4127619601)

[CUDA编程基础(二)—CUDA编程模型（上） - 皮皮鲁的文章 - 知乎 https://zhuanlan.zhihu.com/p/6775703426](https://zhuanlan.zhihu.com/p/6775703426)

[CUDA编程基础(三)—CUDA编程模型（下） - 皮皮鲁的文章 - 知乎 https://zhuanlan.zhihu.com/p/6795161821](https://zhuanlan.zhihu.com/p/6795161821)

[CUDA编程基础(四)—GPU线程调度 - 皮皮鲁的文章 - 知乎 https://zhuanlan.zhihu.com/p/12083951223](https://zhuanlan.zhihu.com/p/12083951223)

[https://www.cnblogs.com/skyfsm/p/9673960.html](https://www.cnblogs.com/skyfsm/p/9673960.html)
[[../并行程序设计基础]]
## 前言

GPU （Graphics Processing Unit）, 翻译成中文是图形处理单元。我们一般会将 GPU 和 CPU 进行类比, 认为两者是同等地位的, 因为两者的名字只相差一个单词。这样的想法是错误的。GPU 和IO 设备 一样, 属于 外接设备, 有时我们也称其为 计算设备。和其它设备一样, 在安装完成 GPU 后, 我们需要安装显卡驱动, 才能保证程序的正常运行。所有的外接设备统一由 CPU 调控, GPU 自然也不例外。只有当 CPU 向 GPU 发出指令信号时, GPU 才会执行, 其它阶段都是不执行的。在硬件架构上, GPU 等价于 CPU + 内存, 可以进行独立的计算。我们可以将 GPU 理解为一个半成品的电脑, 只有 CPU 和 内存, 没有硬盘, 也没有操作系统。

因此, 在 GPU 编程时, 我们将 计算机 称为主机(host), 而 GPU 称为设备(device)。主机 向 设备 发出信号, 设备开始进行运算, 主机 不用等待 设备 运行完成, 可以去执行其它的指令。两者之间是 "异步执行" 的关系。

## 1串行计算与并行计算

首先我们先谈一谈串行计算和并行计算。我们知道，高性能计算的关键利用多核处理器进行并行计算。

当我们求解一个计算机程序任务时，我们很自然的想法就是将该任务分解成一系列小任务，把这些小任务一一完成。在串行计算时，我们的想法就是让我们的处理器每次处理一个计算任务，处理完一个计算任务后再计算下一个任务，直到所有小任务都完成了，那么这个大的程序任务也就完成了

但是串行计算的缺点非常明显，如果我们拥有多核处理器，我们可以利用多核处理器同时处理多个任务时，而且这些小任务并没有关联关系（不需要相互依赖，比如我的计算任务不需要用到你的计算结果），那我们为什么还要使用串行编程呢？为了进一步加快大任务的计算速度，我们可以把一些独立的模块分配到不同的处理器上进行同时计算（这就是并行），最后再将这些结果进行整合，完成一次任务计算。下图就是将一个大的计算任务分解为小任务，然后将独立的小任务分配到不同处理器进行并行计算，最后再通过串行程序把结果汇总完成这次的总的计算任务。

所以，一个程序可不可以进行并行计算，关键就在于我们要分析出该程序可以拆分出哪几个执行模块，这些执行模块哪些是独立的，哪些又是强依赖强耦合的，独立的模块我们可以试着设计并行计算，充分利用多核处理器的优势进一步加速我们的计算任务，强耦合模块我们就使用串行编程，利用串行+并行的编程思路完成一次高性能计算。

### 1.1处理器与数据分类

- 单指令流单数据流（SISD）：传统的单核处理器，每次只能处理一条指令和一块数据。这种架构在早期计算机中非常常见，处理器以线性的方式处理任务，逐条指令执行，效率相对较低。
    
- 单指令流多数据流（SIMD）：在多个数据上并行执行相同的指令操作。简化来理解，假设有两个4维32位浮点数向量加和操作，对于SISD，则需要循环4次，而对于SIMD，如果包含128位（4*32）寄存器，则一次操作即可完成。这种架构擅长处理需要对较大量数据进行相同操作的任务，例如图像处理、矩阵运算和科学计算。（参考：[Single Instruction Multiple Data](http://link.zhihu.com/?target=https%3A//www.sciencedirect.com/topics/computer-science/single-instruction-multiple-data)）
    
- 多指令流单数据流（MISD）：较少见，多条指令流处理同一块数据。
    
- 多指令流多数据流（MIMD）：多核处理器，每条指令流可以独立处理不同的数据块。这种架构是现代多核处理器的基础，允许多个处理器核心独立运行不同的指令流，从而能够并行处理不同的数据块。这种架构非常适合复杂的计算任务，如并行编程和多任务处理。
    
- 单指令流多线程（SIMT）：以上4种一般都是指CPU的并行方式，SIMT是GPU的并行方式，其与SIMD有些类似，也是对多个数据进行并行操作；不同的是，在SIMD中，实际上还是单线程（单核心）处理，只是寄存器和ALU的位宽更宽，一次可以处理更多bits的数据；而在SIMT中，是多个核心并行运行，逻辑上看是同样的指令广播到各个核心独立运行，每个核心都有自己的寄存器，ALU，cache等，是多线程执行；另外GPU的核心数要比CPU高几个数量级，因此并行程度也相对CPU大大增加。（参考：[Single instruction, multiple threads](http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Single_instruction%2C_multiple_threads)）
    

实际上SIMT可以看成是SIMD的一个特例，区别在于SIMT在SIMD基础上支持了条件分支执行以及更高的操作数位宽。关于这点在后面的章节中会有更详细的描述。

### 1.2 从内存角度

- 多处理器共享内存架构（Multiprocessor with Shared Memory）：多个处理器共享一个全局内存空间，所有处理单元可以直接访问同一个内存地址（一般是多个处理器在一块主板上）。这种架构可以减少数据传输的延迟，提高并行计算的效率，但也需要处理竞争和同步问题。
    

![](https://picx.zhimg.com/80/v2-7b3df19c80688ef4408577acc7d39e89_720w.jpg?source=d16d100b)



- 多节点分布式内存架构（Multi-node with Distributed Memory）：每个节点（node）都有自己的本地内存，节点之间通过网络交换数据。这种架构适用于大规模并行计算，但需要有效的通信策略来减少数据传输的开销。这种结构就是我们常说的集群（cluster）。
    

![](https://pic1.zhimg.com/80/v2-06f05bb0abba4ad925ebb30ff293117a_720w.jpg?source=d16d100b)

添加图片注释，不超过 140 字（可选）

### 1.3 从任务逻辑层面划分

- 数据并行：把数据切分成多块，每个线程处理一块数据，每个线程的处理逻辑相同，最后视需要汇总。这种并行方式适用于处理大量数据的任务，例如矩阵运算、图像处理等。CUDA的并行范式从大类上来讲属于数据并行，因为其底层是把数据切分后，通过GPU的多个计算核心并行运行线程进行处理。
    
- 任务并行：把任务切分成很多部分，每个部分完成任务中的一部分逻辑，最后视需要汇总。这种并行方式适用于任务本身可以分解为多个独立子任务的情况，每个子任务可以由不同的线程或进程来执行，互相之间没有依赖或者依赖程度较低。
    

### 1.4 CPU+GPU异构计算

两者各有所长，CPU擅长复杂的逻辑控制，较适合于任务并行；而GPU则有大量的计算核心，相对简单的逻辑控制，适合大规模的数据并行。CPU和GPU之间一般通过系统总线（例如PCIE、NVlink）进行通信。另外目前来说GPU不能脱离CPU独立运行，可以看做是CPU的一个协处理器，通常CPU叫host，GPU叫device。在异构环境下，我们需要对程序进行良好的设计，以使得CPU和GPU能够各展所长，充分发挥系统的整体性能。

![](https://picx.zhimg.com/80/v2-02b39c18b7b588e42d87197795684b79_720w.jpg?source=d16d100b)


---

## 2 GPU硬件概述

GPU的硬件由以下几个关键模块组成：

- 流处理器
    
- 内存（全局的，常量的共享的）
    
- 流处理器簇
    

### 2.1流多处理器SM

GPU并行性依靠流多处理器SM（streaming multiprocessor） 来完成，一个GPU是由多个SM构成的，Fermi架构SM关键资源如下：

- 1、CUDA核心（CUDAcore）：其中包含整数处理单元和单精度浮点数处理单元，用于执行基本的数值运算。不同架构中CUDA core数量不同，这个数量在一定程度上体现了GPU的计算能力（并不是完全决定，还有如时钟频率，内存带宽，指令集等其他影响因素）
    
- 2、共享内存/L1缓存（sharedmemory/L1 cache） ：shared memory可以用于同一个thread block中的线程间互相通信，是可以通过编程读写的。L1 Cache则不能被编程，由GPU决定存放和淘汰内容。这里把Shared Memory和L1 Cache放在一起是因为它们在物理存储上是共用的，可以通过API控制两者的配比。
    
- 3、寄存器文件（RegisterFile）：寄存器文件。存放指令操作数；也有一些特殊寄存器用于存放系统变量，例如grid维度，thread blcok维度，线程id等等。
    
- 4、加载和存储单元（Load/Store Units） ：执行内存（显存、shared memory）读写数据命令。
    
- 5、特殊函数单元（SpecialFunction Unit）：执行一些特殊函数，如sqrt，sin，cos等。
    
- 6、Warps调度（Warps Scheduler：GPU线程调度执行时是以一组线程（warp）为单位的，Warp Scheduler从驻留在SM上的warp中选择合适的warp用于执行。
    
- Dispatch Unit：负责从Warp Scheduler选中的线程中取出要执行的指令发射到CUDA core去执行。
    

![](https://pic1.zhimg.com/80/v2-ee067b77a15b12e1d0d5586f1c2c8617_720w.png?source=d16d100b)


以上这些核心组件是以Fermi架构为例，后续的新架构都是在其基础上的进一步发展，例如更多的CUDA core数量，更大的内存容量，更高的IO带宽，以及增加一些新的组件如Tensor Core，RT Core等。

  

### 2.2GPU的线程调度

上一章介绍过从编程角度看，一个CUDA kernel对应一个grid，一个grid分成若干个thread block，每个thread block中包含若干线程。那么从硬件角度看，GPU在调度Kernel时，粗粒度看分成两个阶段，即SM分配和线程调度执行

线程模型与物理结构：

- 左图线程模型，是在逻辑角度进行分析
    
- 线程模型可以定义成千上万个线程
    
- 网格中的所有线程块需要分配到SM上进行执行
    
- 线程块内的所有线程分配到同一个SM中执行， 但是每个SM上可以被分配多个线程块
    
- 线程块分配到SM中后，会以32个线程为一组 进行分割，每个组成为一个wrap
    
- 右图物理结构，是在硬件角度进行分析，因为硬 件资源是有限的，所以活跃的线程束的数量会受 到SM资源限制
    

![](https://pic1.zhimg.com/80/v2-69da399aefe9bb403d005e29f0a15a05_720w.png?source=d16d100b)

添加图片注释，不超过 140 字（可选）

线程束

CUDA 采用单指令多线程SIMT架构管理执行线 程，每32个为一组，构成一个线程束。 同一个线程块中相邻的 32个线程构成一个线程 束 具体地说，一个线程块中第 0 到第 31 个线程 属于第 0 个线程束，第 32 到第 63 个线程 属 于第 1 个线程束，依此类推。

![](https://pica.zhimg.com/80/v2-2b935d50d699909150857b12c24ddf2c_720w.png?source=d16d100b)


- 每个线程束中只能包含同一线程块中的线程
    
- 每个线程束包含32个线程
    
- 线程束是GPU硬件上真正的做到了并行
    
- 线程束数量 =ceil（线程块中的线程数/32）
    

![](https://pica.zhimg.com/80/v2-822f77aef33eab3fa64a1285783257be_720w.png?source=d16d100b)


### 2.3CUDA内存模型

- 寄存器（register）
    
- 共享内存（sharedmemory）
    
- 本地内存（local memory ）
    
- 常量内存（constant memory ）
    
- 纹理内存（texture memory）
    
- 全局内存（global memory）
    

![](https://picx.zhimg.com/80/v2-623d1a85484ca49c1fd2e0ab28b7d38c_720w.png?source=d16d100b)

添加图片注释，不超过 140 字（可选）

![](https://picx.zhimg.com/80/v2-787b3838185e4ee8c5c48814e49e44eb_720w.png?source=d16d100b)

CUDA内存和它们的主要特征

寄存器

- 寄存器内存在片上（on-chip），具有GPU上最 快的访问速度，但是数量有限，属于GPU的稀 缺资源；
    
- 寄存器仅可在线程内可见，生命周期也与所属 线程一致；
    
- 核函数中定义的不加任何限定符的变量一般存 放在寄存器中；
    
- 内建变量存放于寄存器中，如gridDim、 blockDim、blockIdx等；
    
- 核函数中定义的不加任何限定符的数组有可能 存在于寄存器中，但也有可能存在于本地内存 中；
    

本地内存

- 寄存器放不下的内存会存放在本地内存
    
- 索引值不能在编译时确定的数组存放于本地内存
    
- 可能占用大量寄存器空间的较大本地结构体和数组
    
- 任何不满足核函数寄存器限定条件的变量
    

共享内存

- 共享内存在片上（on-chip），与本地内存和全 局内存相比具有更高的带宽和更低的延迟；
    
- 共享内存中的数据在线程块内所有线程可见， 可用线程间通信，共享内存的生命周期也与所 属线程块一致；
    
- 使用__shared__修饰的变量存放于共享内存中， 共享内存可定义动态与静态两种；
    
- 每个SM的共享内存数量是一定的，也就是说， 如果在单个线程块中分配过度的共享内存，将 会限制活跃线程束的数量； 访问共享内存必须加入同步机制： 线程块内同步 void__syncthreads();
    

![](https://picx.zhimg.com/80/v2-72f896b236704ffaf3fa61eedd22d2fc_720w.png?source=d16d100b)

不同计算能力的架构对应不同的大小

全局内存

- 全局内存在片外。 特点：容量最大，延迟最大，使用最多；
    
- 全局内存中的数据所有线程可见，Host端可见， 且具有与程序相同的生命周期
    
- 动态全局内存： 主机代码中使用CUDA运行时APIcudaMalloc动态声明内存空 间，由cudaFree释放全局内存
    
- 静态全局内存： 使用__device__关键字静态声明全局内存
    

![](https://pica.zhimg.com/80/v2-a211b3f4c58c29151032cd622cd9a667_720w.png?source=d16d100b)



经常访问的数据由全局内存（global memory）搬移到共享内存（shared memory），提 高访问效率； 改变全局内存访问内存的内存事务方式，提高数据访问的带宽

共享内存变量修饰符：__shared__ 静态共享内存声明：__shared__float tile[size, size];

静态共享内存作用域：

- 1、核函数中声明，静态共享内存作用域局限在这个核函数中；
    
- 2、文件核函数外声明，静态共享内存作用域对所有核函数有效。 静态共享内存在编译时就要确定内存大小
    

常量内存

  

GPU的缓存